#!/bin/bash
# GlusterFS tips and trick and caveats



# Reset an Option to default parameters
sudo gluster volume reset R1 <option>






# When Getting this error: 
# (is not in 'Peer in Cluster' state)

1: sudo gluster peer detach <hostname|ip>

2: Add all the Servers to the /etc/hosts file in all Servers

192.168.0.11  S1 S1
192.168.0.12  S2 S2
192.168.0.13  S3 S3
192.168.0.14  S4 S4

# Note: using hostnames is better
3:sudo gluster peer probe <hostname|ip> 

4: wait for the motherfucker to add the peer to cluster , 
	or else you will get the dreaded error (Peer not in Cluster)

5: Make sure the peer was added correctly by 
	issuing "sudo gluster peer status" and getting 
	(Peer in Cluster (connected))

















# Brick Restoration - Replace Crashed Server

#This procedure is for replacing a failed server, IF your newly installed server has the same hostname as the failed one:

#(If your new server will have a different hostname, see this article instead.)

#For purposes of this example, the server that crashed will be server3 and the other servers will be server1 and server2

#    On both server1 and server2, make sure hostname server3 resolves to the correct IP address of the new replacement server.
#    On either server1 or server2, do

    grep server3 /var/lib/glusterd/peers/*

#    This will return a uuid followed by ":hostname1=server3"
#    On server3, make sure glusterd is stopped, then do

    echo UUID={uuid from previous step}>/var/lib/glusterd/glusterd.info

#    On server3:
#        make sure that all brick directories are created/mounted
#        start glusterd
#        peer probe one of the existing servers
#        restart glusterd, check that full peer list has been populated using

        gluster peer status

#        (if peers are missing, probe them explicitly, then restart glusterd again)
#        check that full volume configuration has been populated using

        gluster volume info

#        if volume configuration is missing, do

        gluster volume sync server1 all

#    add the volume id(s) to the new brick(s) (do this for each volume/brick on this server)

    vol=myvol
    brick=/mnt/myvol/brick1
    setfattr -n trusted.glusterfs.volume-id \
      -v 0x$(grep volume-id /var/lib/glusterd/vols/$vol/info \
      | cut -d= -f2 | sed 's/-//g') $brick

#    restart glusterd

#This will restore the configuration for this server.

#If this is a replica brick, restore the data with

 gluster volume heal $vol full

#######################################################################

# Resources

http://serverfault.com/questions/531359/why-cant-i-create-this-gluster-volume
https://bugzilla.redhat.com/show_bug.cgi?id=1055928
http://gluster.org/community/documentation/index.php/Gluster_3.2:_Adding_Servers_to_Trusted_Storage_Pool
